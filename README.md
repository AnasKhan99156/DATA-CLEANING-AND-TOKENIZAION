# DATA-CLEANING-AND-TOKENIZAION

In this Repository you  will find the code for 

i) Data Cleaning:

    Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. When combining multiple data sources, there are many opportunities for data to be duplicated or mislabeled.

ii) Data Tokenization by words:

    Tokenisation is the process of breaking up a given text into units called tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks may be discarded.

